---
title: LSTM 정리
description:
categories:
tags:
---


원 논문 : Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735–1780.

“밑바닥부터 시작하는 딥러닝 2”를 옮겼습니다. 그림은 직접 그렸습니다.


---

# RNN의 문제점은 무엇일까?
언어 모델은 현재까지 앞 단어들이 주어지면 다음 단어를 예측한다. 그런데 아래와 같이 긴 문장이 주어진 경우를 보자.

> Tom was watching TV in his room. Mary came into the room. Mary said hi to [?]

[?]에 들어갈 단어는 당연히 Tom이다. 모델이 해야할 일은 무엇일까? 앞의 모든 단어가 주어지면 Tom을 가장 높은 확률로 예측하도록 학습해야 한다. 학습 시에 모델에게 정답인 ‘Tom’이 주어졌다고 해보자. 그럼 loss를 역전파하여 파라미터를 업데이트 해야 할텐데, RNN은 이 과정에서 순탄치 못하다. RNN은 맨 앞에 있는 Tom까지 의미 있는 기울기를 전달하기 힘들어하기 때문이다.

왜 그럴까? 결론적으로는 tanh함수와 행렬곱 연산때문에 그렇다. 위의 예에서는 단어가 약 20개 가까이 되는데, 그만큼 tanh 계산도 20번 존재한다. tanh의 미분값은 0~1사이로, 역전파로 한 번 지날 때마다 무조건 기울기가 작아진다. 행렬곱 계산도 마찬가지다. 위의 예로 치면 약 20번 동안 같은 행렬을 계속 곱하는 작업을 하게 된다. 이 행렬곱 노드에서 20번 역전파 하게 되면 기울기 폭발 또는 소실이 쉽게 일어난다.

기울기 폭발의 전통적 해결법은 기울기 클리핑(gradient clipping)이다. 기울기의 L2 norm이 특정 threshold를 넘으면 다시 줄여주는 단순한 방법이다.

# 그렇다면 기울기 소실은?

이를 해결하려면 RNN의 근본부터 뜯어 고쳐야 하며, 게이트를 추가한 모델인 LSTM과 GRU가 대표적이다. 게이트란 말 그대로 ‘문’이라는 뜻으로, 핵심은 기억 셀이라는 것이 여러 개의 게이트를 통과하며 어떤 정보를 기억할 지, 망각할 지를 학습하게 된다.

![](/assets/images/lstm/RNN과 LSTM 비교.png)

# LSTM의 핵심은 기억 셀(memory cell, cell state, c)이다.

이 기억 셀은 외부로는 출력 되지 않는다. 단지 망각해야할 정보, 기억해야 할 정보와 계속 연산해 나간다.

![](/assets/images/lstm/LSTM 셀.png)

기억 셀 기준으로 어떤 연산이 이루어지는지 살펴보면 편하다. cₜ₋₁이 cₜ로 되는 과정은 위 그림과 같다. 일단 모든 연산이 이전 hidden state인 hₜ₋₁과 현재 인풋 단어 x로부터 시작한다. 그럼 이제 저 구름 안에서는 어떤 연산이 벌어지는 지를 하나하나 보면 된다.

## 1. 망각의 문, forget gate

$c_{t-1}$가 통과하는 첫 번째 게이트는 망각의 문이다. f라는 게이트는 아래와 같은 수식으로 얻어진다.

![](/assets/images/lstm/forget gate.png)

$$f=\sigma(x_tW_x^f + h_{t-1}W_h^f + b^f)$$

이것이 forget gate이다. 이를 과 원소별 곱을 한다. $c_{t-1}$과 원소별 곱을 한다.